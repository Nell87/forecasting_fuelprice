{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Libraries\n",
    "import requests \n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from decimal import Decimal\n",
    "from datetime import datetime, timedelta\n",
    "from io import StringIO\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "\n",
    "os.environ[\"AWS_PROFILE\"] = (\"mlops\") # fill in with your AWS profile. More info: https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/setup.html#setup-credentials\n",
    "os.environ['AWS_DEFAULT_REGION'] = \"eu-west-1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape weekly prices (Year has four digits and month one/two digits)\n",
    "def fuel_scraper(year, month):\n",
    "\n",
    "    # Let's make a request to check the status\n",
    "    response = requests.get('https://es.fuelo.net/calendar/month/' + str(year) +  \"/\" + str(month))\n",
    "    status_code = (response.status_code)    \n",
    "\n",
    "    if status_code != 200:\n",
    "        #print( \"The status code is not 200\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    else:\n",
    "        # Extract content\n",
    "        soup = bs(response.content,'html.parser')\n",
    "\n",
    "        # Prepare the dataframe\n",
    "        df=pd.DataFrame(columns=[\"Day\", \"Diesel\"])\n",
    "        \n",
    "        # Scraper\n",
    "        i = 0\n",
    "        for td in soup.table.find_all('td'):\n",
    "            if (td.text.find(\"DSL\")>-1):\n",
    "                    pattern = \" \" + \".*\"\n",
    "                    day = re.sub(pattern, '', td.get_text(strip=False) )\n",
    "                    pattern  = \".*\" + \"DSL:\" \n",
    "                    price = re.sub(pattern, '', td.get_text(strip=False) )\n",
    "                    pattern = \"â‚¬/l\" + \".*\"\n",
    "                    price = re.sub(pattern, '', price )\n",
    "                    df.at[i, \"Day\"] = day\n",
    "                    df.at[i, \"Diesel\"] = price\n",
    "                    i = i+1\n",
    "\n",
    "        # Add Date Column\n",
    "        df['Date'] = pd.to_datetime(dict(year=year, month=month, day=df.Day))\n",
    "\n",
    "        # Remove week columns\n",
    "        df = df.drop('Day', axis=1)\n",
    "\n",
    "        # Reorganize columns\n",
    "        df = df[['Date', 'Diesel']]\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_s3(bucket, new_data):\n",
    "    s3 = boto3.client('s3')\n",
    "    csv_buffer = StringIO()\n",
    "    new_data.to_csv(csv_buffer, index=False)\n",
    "\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    s3_resource.Object(bucket, 'new_data.csv').put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_datasets_S3():\n",
    "    bucket = 'gas-prices-project'\n",
    "    filename_1 = 'data.csv'\n",
    "    filename_2 = 'new_data.csv'\n",
    "    \n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    first_obj = s3.get_object(Bucket= bucket, Key= filename_1)\n",
    "    second_obj = s3.get_object(Bucket= bucket, Key= filename_2)\n",
    "    \n",
    "    first_df = pd.read_csv(first_obj['Body'])\n",
    "    second_df = pd.read_csv(second_obj['Body'])\n",
    "    \n",
    "    concat_data = pd.concat([first_df, second_df]) \n",
    "    concat_data = concat_data.drop_duplicates(subset=None, keep=\"first\", inplace=False)\n",
    "\n",
    "    return concat_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraper current month\n",
    "currentYear = datetime.now().year\n",
    "currentMonth = datetime.now().month\n",
    "dataset = fuel_scraper(2023, 3)\n",
    "\n",
    "# Scraper previous month\n",
    "if currentMonth > 1:\n",
    "    currentMonth = currentMonth -1\n",
    "else:\n",
    "    currentMonth = 12\n",
    "    currentYear = currentYear - 1\n",
    "\n",
    "dataset_prev = fuel_scraper(currentYear, currentMonth)\n",
    "dataset = pd.concat([dataset_prev, dataset]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset.empty == False:\n",
    "    # Upload S3\n",
    "    upload_s3(\"gas-prices-project\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset.empty == False:\n",
    "    # Merge\n",
    "    concat_data = merge_datasets_S3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset.empty == False:\n",
    "# Upload S3\n",
    "    upload_s3(\"gas-prices-project\", concat_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset.empty == False:\n",
    "# Rename\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.Object('gas-prices-project','data.csv').delete()\n",
    "    s3.Object('gas-prices-project','data.csv').copy_from(CopySource='gas-prices-project/new_data.csv')\n",
    "    s3.Object('gas-prices-project','new_data.csv').delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
